# -*- coding: utf-8 -*-
"""NER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rVBHSrAJOvoznPJCwfkBCaAIuvTasa9v
"""

import pandas as pd
import numpy as np
from tensorflow import keras
import tensorflow as tf
import matplotlib.pyplot as plt

df = pd.read_csv('NER_Dataset.csv',error_bad_lines=False,engine='python')

df.shape

df.head()

df.isnull().sum()

x = list(df['Word'])
y = list(df['Tag'])

df['splited_word'] = df['Word']

print(type(y))

"""### Pre processing Tags"""

def preprocess_data(df):
    for i in range(len(df)):
        pos = ast.literal_eval(df['POS'][i])
        tags = ast.literal_eval(df['Tag'][i])
        word = ast.literal_eval(df['Word'][i])
        df['POS'][i] = [str(word) for word in pos]
        df['Tag'][i] = [str(word.upper()) for word in tags]
        df['Word'][i] = [str(word) for word in word]
    return df

import ast

df = preprocess_data(df)
df.head()

Tag = df['Tag'].to_list()

import ast

flattened_set_train = set(value for sublist in df.Tag.to_list() for value in sublist)

print(len(flattened_set_train))

target_tag = list(flattened_set_train)
print(target_tag)

num_tags = len(target_tag)
print(num_tags)

tags_map = {tag:i for i,tag in enumerate(flattened_set_train)}
print(tags_map)

reverse_tag_map = {v:k for k,v in tags_map.items()}

"""## Encoded tags -- tags with number"""

encoded_tags = [[tags_map[w] for w in tag] for tag in Tag]

df['Word'][0]

encoded_tags[0]

"""## pre processing text"""

df['tokenized_word'] = df['Word']

df.head()

sentence_list = df['Word'].tolist()

tokeniser= tf.keras.preprocessing.text.Tokenizer(lower=False,filters='')

tokeniser.fit_on_texts(sentence_list)

print("Vocab size of Tokeniser ",len(tokeniser.word_index)+1) ## Adding one since 0 is reserved for padding

encoded_sentence=tokeniser.texts_to_sequences(sentence_list)
print("original sentence \n",sentence_list[0])
print("encoded sentence \n",encoded_sentence[0])

max_words = max(len(string.split()) for string in df['splited_word'])
print(max_words)

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

max_len = 128

padded_encoded_sentence = pad_sequences(maxlen = max_len,sequences=encoded_sentence,padding = 'post',value=0)
padded_encoded_tags = pad_sequences(maxlen=max_len,sequences=encoded_tags,padding='post',value=tags_map['O'])

print("Shape of Encoded Sentence: ",padded_encoded_sentence.shape)
print("Shape of Encoded Labels: ",padded_encoded_tags.shape)

print("First Encoded Sentence Without Padding:\n",encoded_sentence[0])
print("First Encoded Sentence with padding:\n",padded_encoded_sentence[0])
print("First Sentence Encoded Label without Padding:\n",encoded_tags[0])
print("First Sentence Encoded Label with Padding:\n",padded_encoded_tags[0])

target= [to_categorical(i,num_classes = num_tags) for i in  padded_encoded_tags]
print("Shape of Labels  after converting to Categorical for first sentence: ",target[0].shape)

"""## Splitting data"""

from sklearn.model_selection import train_test_split
x_train,x_val_test,y_train,y_val_test = train_test_split(padded_encoded_sentence,target,test_size = 0.3,random_state=42)
x_val,x_test,y_val,y_test = train_test_split(x_val_test,y_val_test,test_size=0.2,random_state=42)
print("Input Train Data Shape: ",x_train.shape)
print("Train Labels Length: ",len(y_train))
print("Input Test Data Shape: ",x_test.shape)
print("Test Labels Length: ",len(y_test))

print("Input Validation Data Shape: ",x_val.shape)
print("Validation Labels Length: ",len(y_val))

print("First Sentence in Training Data: ",x_train[0])
print("First sentence Label: ",y_train[0])
print("Shape of First Sentence -Train: ",x_train[0].shape)
print("Shape of First Sentence Label  -Train: ",y_train[0].shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding,LSTM,Dense,Bidirectional,TimeDistributed
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint

model=Sequential()
model.add(Embedding(input_dim=len(tokeniser.word_index)+1,output_dim=32,input_length=max_len))
model.add(Bidirectional(LSTM(units=128,return_sequences=True,recurrent_dropout=0.2)))
model.add(TimeDistributed(Dense(num_tags,activation='softmax')))
model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.001),metrics=['accuracy'])
model.summary()
checkpointer = ModelCheckpoint(filepath="model.h5", verbose=0, save_best_only=True)
early_stopping = EarlyStopping(monitor='val_loss', patience=5)
history=model.fit(x_train,np.array(y_train),batch_size=32,epochs=5,validation_data=(x_val,np.array(y_val)),verbose=1,callbacks=[early_stopping,checkpointer])

preds=model.predict(x_test) ## Predict using model on Test Data

def evaluatePredictions(test_data,preds,actual_preds):
    print("Shape of Test Data Array",test_data.shape)
    y_actual=np.argmax(np.array(actual_preds),axis=2)
    y_pred=np.argmax(preds,axis=2)
    num_test_data=test_data.shape[0]
    print("Number of Test Data Points ",num_test_data)
    data=pd.DataFrame()
    df_list=[]
    for i in range(num_test_data):
        test_str=list(test_data[i])
        df=pd.DataFrame()
        df['test_tokens']=test_str
        df['tokens']=df['test_tokens'].apply(lambda x:tokeniser.index_word[x] if x!=0 else '<PAD>')
        df['actual_target_index']=list(y_actual[i])
        df['pred_target_index']=list(y_pred[i])
        df['actual_target_tag']=df['actual_target_index'].apply(lambda x:reverse_tag_map[x])
        df['pred_target_tag']=df['pred_target_index'].apply(lambda x:reverse_tag_map[x])
        df['id']=i+1
        df_list.append(df)
    data=pd.concat(df_list)
    pred_data=data[data['tokens']!='<PAD>']
    accuracy=pred_data[pred_data['actual_target_tag']==pred_data['pred_target_tag']].shape[0]/pred_data.shape[0]


    return pred_data,accuracy

pred_data,accuracy=evaluatePredictions(x_test,preds,y_test)

y_pred=pred_data['pred_target_tag'].tolist()
y_actual=pred_data['actual_target_tag'].tolist()

pred_data.head(40)

